{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pa49bUnKyRgF"
   },
   "source": [
    "## River forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XVhK72Pu1cJL"
   },
   "source": [
    "## Setup packages\n",
    "Note - Operational version available at: https://mybinder.org/v2/gh/scottmreed/GaugeGeek/main "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-21T02:33:33.688365Z",
     "iopub.status.busy": "2021-01-21T02:33:33.687646Z",
     "iopub.status.idle": "2021-01-21T02:33:40.788210Z",
     "shell.execute_reply": "2021-01-21T02:33:40.787582Z"
    },
    "id": "7rZnJaGTWQw0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import pytest\n",
    "import requests\n",
    "\n",
    "import IPython\n",
    "import IPython.display\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import scipy as sp\n",
    "from pandas import concat\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import median_absolute_error\n",
    "\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.utils import Bunch\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.neural_network import MLPRegressor \n",
    "from sklearn.svm import SVC\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (8, 6)\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "\n",
    "IPython.display.clear_output()\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change directory if needed\n",
    "%cd ./datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TokBlnUhWFw9"
   },
   "source": [
    "## Check the weather "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-21T02:33:40.793352Z",
     "iopub.status.busy": "2021-01-21T02:33:40.792638Z",
     "iopub.status.idle": "2021-01-21T02:33:41.856681Z",
     "shell.execute_reply": "2021-01-21T02:33:41.855926Z"
    },
    "id": "xyv_i85IWInT"
   },
   "outputs": [],
   "source": [
    "# You can upload csv data that NOAA sends you by email from https://www.ncdc.noaa.gov/cdo-web/\n",
    "\n",
    "dataset = pd.read_csv('DenverWeatherHistory.csv', index_col=2)\n",
    "remove = ['TOBS', 'STATION','NAME']\n",
    "weather_df = dataset[dataset.columns.difference(remove)]\n",
    "dataset.index.name = 'Date'\n",
    "print(weather_df.describe())\n",
    "\n",
    "# if you want weather for a different zipcode or date, you can use the code below\n",
    "# date_time = pd.to_datetime(weather_df.pop('DATE'), format='%d.%m.%Y')\n",
    "# timestamp_s = date_time.map(datetime.datetime.timestamp)\n",
    "\n",
    "# or download directly from NOAA API\n",
    "# from noaa_sdk import NOAA\n",
    "# n = NOAA()\n",
    "# observations = n.get_observations('80204','US',start='2021-02-11', end='2021-02-12')\n",
    "# for observation in observations:\n",
    "#     temp = (observation['temperature']['value'])\n",
    "#     time_stamp = (observation['timestamp'])\n",
    "#     print(time_stamp, temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VdbOWXiTWM2T"
   },
   "source": [
    "# Load river data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-21T02:33:42.829510Z",
     "iopub.status.busy": "2021-01-21T02:33:42.822999Z",
     "iopub.status.idle": "2021-01-21T02:33:42.833554Z",
     "shell.execute_reply": "2021-01-21T02:33:42.834000Z"
    },
    "id": "ojHE-iCCWIhz"
   },
   "outputs": [],
   "source": [
    "# load water flow data \n",
    "# add your token if you have an account with Denver Water and then you can download more data per day\n",
    "token = '####'\n",
    "# reduce the page size if you are concerned about running out of requests\n",
    "page_Size = 50000\n",
    "\n",
    "def Load_gauge(gauge_label, gauge_name, start_date, end_date):\n",
    "    flows=[]\n",
    "    dates=[]\n",
    "\n",
    "    URL = f'https://dwr.state.co.us/Rest/GET/api/v2/surfacewater/surfacewatertsday/?format=json&dateFormat=spaceSepToSeconds&fields=measDate%2Cvalue&abbrev={gauge_name}&min-measDate={start_date}&max-measDate={end_date}&pageSize={page_Size}'\n",
    "    urlData = requests.get(URL).content\n",
    "    rawData = pd.read_json(urlData.decode('utf-8'))\n",
    "    data=(rawData.ResultList)\n",
    "\n",
    "    for count,entry in enumerate(data):\n",
    "        dates.append(data.iloc[count]['measDate'])\n",
    "        flows.append(data.iloc[count]['value'])\n",
    "        element = data.iloc[count]\n",
    "\n",
    "    date_series = pd.Series(dates) \n",
    "    flow_series = pd.Series(flows) \n",
    "\n",
    "    frame = {'Date': date_series, 'cfs_'+gauge_label: flow_series} \n",
    "    Gauge_Data = pd.DataFrame(frame) \n",
    "\n",
    "    Gauge_Data['Date'] = pd.to_datetime(Gauge_Data['Date'])\n",
    "    Gauge_Data = Gauge_Data.set_index('Date')\n",
    "    \n",
    "    return Gauge_Data\n",
    "\n",
    "name_1 = 'roberts' # red\n",
    "name_2 = 'deckers' # blue\n",
    "name_3 = 'grant' # green\n",
    "start_date = '01%2F01%2F2010' # 01%2F01%2F2010 means January 1, 2010\n",
    "end_date = '1%2F31%2F2021'\n",
    "begin_date = start_date.replace('%2F','-')\n",
    "until_date = end_date.replace('%2F','-')\n",
    "gauge_1 = Load_gauge(name_1,'ROBTUNCO', start_date, end_date)\n",
    "gauge_2 = Load_gauge(name_2,'PLASPLCO', start_date, end_date)\n",
    "gauge_3 = Load_gauge(name_3,'PLAGRACO', start_date, end_date)\n",
    "\n",
    "# You can test out other API calls here: https://dwr.state.co.us/Rest/GET/Help/SurfaceWaterTSDayGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print seasonal view as overlay to preview data\n",
    "\n",
    "def plot_preview(gauge1, gauge2, gauge3, start, stop):\n",
    "    plt.figure(figsize=(14, 4))\n",
    "    plt.title('gauges')\n",
    "    plot_key = 'red is gauge 1, blue is gauge 2, green is gauge 3'\n",
    "    plt.text(gauge_1.index[0],max(gauge_2.values),plot_key)\n",
    "    plt.plot(gauge1[start:stop], color='red')\n",
    "    plt.plot(gauge2[start:stop], color='blue')\n",
    "    plt.plot(gauge3[start:stop], color='green')\n",
    "\n",
    "plot_preview(gauge_1,gauge_2,gauge_3, 0, len(gauge_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "# %cd datasets\n",
    "Image(\"./plattecamp.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine river gauges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge River data and combine with weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def day_shift(data, col_names, days_in=1, days_out=1):\n",
    "    n_vars = data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "\n",
    "    for i in range(days_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [(f'{col_names[j]}%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    for i in range(0, days_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [(f'{col_names[j]}') for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [(f'{col_names[j]}(t+%d)' % (i)) for j in range(n_vars)]\n",
    "    agg = concat(cols, axis=1) \n",
    "    agg.columns = names\n",
    "    return agg\n",
    "\n",
    "combined_gauges = pd.concat([gauge_1, gauge_2, gauge_3], axis=1)\n",
    "combined_gauges_index = combined_gauges.index\n",
    "col_names = combined_gauges.columns.values.tolist()\n",
    "values = combined_gauges.values\n",
    "\n",
    "# days_out is number of shifts\n",
    "combo_gauge = day_shift(values, col_names, 0, 3)\n",
    "combo_gauge.index = combined_gauges_index\n",
    "print(combo_gauge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add weather to array\n",
    "wet_gauges = weather_df.join(combo_gauge, how='outer')\n",
    "wet_gauges = wet_gauges.dropna()\n",
    "wet_gauges.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age the weather data by 1 day. This stands in as what the weather forecast probably would have been\n",
    "# For example if the high temp on Sat is 60 the forecasted high temp on Fri for Sat was prob ~60\n",
    "# Need to loop, functionalize, and add more days\n",
    "days_back = 2\n",
    "wet_gauges.loc[:,f'Tmax(t+{days_back})'] = wet_gauges.loc[:,'TMAX'].shift(-days_back)\n",
    "wet_gauges.loc[:,f'Tmin(t+{days_back})'] = wet_gauges.loc[:,'TMIN'].shift(-days_back)\n",
    "# wet_gauges = wet_gauges.dropna()\n",
    "\n",
    "wet_gauges.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse seasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into 3 'seasons' and pick one to move forward with\n",
    "# Different models might work better during different seasons.\n",
    "# The number is the month number so spring is 5 to 6 or May and June.\n",
    "wet_gauges_spring = wet_gauges[wet_gauges.index.month > 4]\n",
    "wet_gauges_spring = wet_gauges_spring[wet_gauges_spring.index.month < 7]\n",
    "wet_gauges_summer = wet_gauges[wet_gauges.index.month > 6]\n",
    "wet_gauges_summer = wet_gauges_summer[wet_gauges_summer.index.month < 9]\n",
    "wet_gauges_fall = wet_gauges[wet_gauges.index.month >8]\n",
    "wet_gauges_fall = wet_gauges_fall[wet_gauges_fall.index.month < 11]\n",
    "\n",
    "wet_gauges_fall.describe()\n",
    "season_selected = wet_gauges_fall\n",
    "season = ('fall')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take a look at the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Heatmap shows correlatations between each variable \n",
    "The diagonal is fully correlated because that is a self-correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This shows how correalted two inputs are.\n",
    "# It can preview whether an input you selected is providing unique information to the model. \n",
    "def show_heatmap(data):\n",
    "    plt.matshow(data.corr())\n",
    "#     plt.figure(5,5)\n",
    "    plt.xticks(range(data.shape[1]), data.columns, fontsize=14, rotation=90)\n",
    "    plt.gca().xaxis.tick_bottom()\n",
    "    plt.yticks(range(data.shape[1]), data.columns, fontsize=14)\n",
    "\n",
    "    cb = plt.colorbar()\n",
    "    cb.ax.tick_params(labelsize=14)\n",
    "    plt.title(\"Feature Correlation Heatmap\", fontsize=14)\n",
    "    plt.show()\n",
    "\n",
    "show_heatmap(season_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get into the weeds and create a grid of Axes such that each variable is shared \n",
    "# across the y-axes across a single row and the x-axes across a single column. \n",
    "# The diagonal plots are  a univariate distribution plots by histogram \n",
    "# showing the range of values for that given variable.\n",
    "# The axes labels may be hard to read but they match the heatmap above\n",
    "# This is slow and can be skipped.\n",
    "\n",
    "_ = sns.pairplot(season_selected, diag_kind = 'hist')\n",
    "# could bootstrap out outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a better method for viewing the distribution of values for each variable as shown on the diagonal above\n",
    "season_selected_mean = season_selected.mean()\n",
    "season_selected_std = season_selected.std()\n",
    "df_std = (season_selected - season_selected_mean) / season_selected_std\n",
    "df_std = df_std.melt(var_name='Column', value_name='Normalized')\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = sns.violinplot(x='Column', y='Normalized', data=df_std)\n",
    "_ = ax.set_xticklabels(season_selected.keys(), rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G6ufs8kk9JQw"
   },
   "source": [
    "### Create a \"bunch\" of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Bunch the data\n",
    "# isolate the 'cfs' column you want to predict as target\n",
    "predict = name_3\n",
    "print(f'this is the gauge being used for forecast: cfs_{predict}')\n",
    "col_names = season_selected.columns.tolist()\n",
    "col_names.remove(f'cfs_{predict}')\n",
    "\n",
    "test_df2_combo = season_selected\n",
    "test_df2_nonflow = season_selected[col_names].copy()\n",
    "\n",
    "test_df2_flow = test_df2_combo[f'cfs_{predict}']\n",
    "\n",
    "test_df2_nonflow.index = pd.RangeIndex(len(test_df2_nonflow.index))\n",
    "test_df2_flow.index = pd.RangeIndex(len(test_df2_flow.index))\n",
    "\n",
    "bunched_data = Bunch(data=test_df2_nonflow, target=test_df2_flow)\n",
    "\n",
    "X = bunched_data.data\n",
    "y = bunched_data.target.values.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into test and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The training set is applied to train the model, finding\n",
    "# the optimal weights, or coefficients, for linear regression.\n",
    "# The test set is used for an unbiased evaluation of the final model. \n",
    "\n",
    "# Defaults to 75% train and 25% test\n",
    "# Could parse it out by years or seasons\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=None, shuffle=False\n",
    ")\n",
    "\n",
    "rows = (test_df2_combo.index)\n",
    "n = (len(rows))\n",
    "test_rows = rows[int(n*.75):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to start some machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Lasso is a linear regression technique. It assumes a linear relationship between the\n",
    "# output variable, river flow in cfs, and each of the input variables.\n",
    "# The model it develops applies a penalty to including variables that\n",
    "# do not help the model, going so far as to eliminate variables. This way if we added a variable\n",
    "# that doesn't help the model it is discarded.\n",
    "\n",
    "# this blog does a really good job of describing this and other machine learning approaches: https://machinelearningmastery.com\n",
    "\n",
    "preprocessor1 = make_column_transformer(\n",
    "        (StandardScaler(), ['TMAX']),\n",
    "        remainder=StandardScaler(),verbose=True)\n",
    "\n",
    "model2 = make_pipeline(\n",
    "    preprocessor1,\n",
    "    TransformedTargetRegressor(\n",
    "        regressor=LassoCV(alphas=np.logspace(-10, 10, 21), max_iter=100000),\n",
    "    ),)\n",
    "\n",
    "_ = model2.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model2.predict(X_train)\n",
    "mae = median_absolute_error(y_train, y_pred)\n",
    "string_score = f'Mean Avg Error on training set: {mae:.2f} cfs'\n",
    "y_pred = model2.predict(X_test)\n",
    "mae = median_absolute_error(y_test, y_pred)\n",
    "string_score += f'\\nMean Avg Error on testing set: {mae:.2f} cfs'\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 3))\n",
    "plt.scatter(y_test, y_pred)\n",
    "ax.plot([0, 1], [0, 1], transform=ax.transAxes, ls=\"--\", c=\"red\")\n",
    "\n",
    "plt.text(20,400,string_score)\n",
    "\n",
    "plt.title(f'Predictions for S. Platte at:{predict} in {season} tested on: {begin_date} to {until_date}')\n",
    "plt.ylabel('Predicted flow [cfs]')\n",
    "plt.xlabel('Real Flow [cfs]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the weighting for the model. \n",
    "# They show how each variable in the model predicted flow at the gauge you are predicting. A large bar to the right means that\n",
    "# if there was a large increase in that variable on that day there would be a large increase in flow at the site you are predicting. \n",
    "coefs = pd.DataFrame(\n",
    "    model2.named_steps['transformedtargetregressor'].regressor_.coef_,\n",
    "    columns=['Coefficients'], index=bunched_data.data.columns\n",
    ")\n",
    "coefs.plot(kind='barh', figsize=(9, 7))\n",
    "string_score = f'Weightings for {predict} in {season} model on : {begin_date} to {until_date}'\n",
    "string_score += f'\\nMean Avg Error on test data: {mae:.2f} cfs'\n",
    "plt.title(string_score)\n",
    "plt.axvline(x=0, color='.5')\n",
    "plt.subplots_adjust(left=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is a simple time series plot\n",
    "# Change start and stop to plot other time periods\n",
    "# Remember there are diconotinuities in the data\n",
    "\n",
    "plt.title('Comparison of real(blue) to predicted(red)')\n",
    "plt.figure(figsize=(14, 4))\n",
    "plot_key = 'blue is real, red is predicted'\n",
    "plt.title('Comparison of real to predicted')\n",
    "start = 0\n",
    "stop = 40\n",
    "plt.scatter(test_rows[start:stop],y_pred[start:stop], color = 'red')\n",
    "plt.plot(test_rows[start:stop],y_test[start:stop])\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "time_series.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "my-rdkit-env",
   "language": "python",
   "name": "my-rdkit-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}